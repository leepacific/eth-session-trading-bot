#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë° ê²€ì¦ í…ŒìŠ¤íŠ¸ êµ¬í˜„
- ìµœì í™” ì†ë„ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§ í…ŒìŠ¤íŠ¸
- íˆìŠ¤í† ë¦¬ì»¬ ë°±í…ŒìŠ¤íŠ¸ ë¹„êµ í…ŒìŠ¤íŠ¸
- ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê²€ì¦ í…ŒìŠ¤íŠ¸
"""

import unittest
import time
import psutil
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import gc
import threading
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸í•  ëª¨ë“ˆë“¤ import
from optimization_pipeline import OptimizationPipeline, PipelineConfig
from performance_evaluator import PerformanceEvaluator
from statistical_validator import StatisticalValidator
from kelly_position_sizer import KellyPositionSizer
from performance_optimizer import PerformanceOptimizer, PerformanceConfig
from realtime_monitoring_system import RealtimeMonitor, MonitoringConfig

class TestOptimizationSpeedBenchmark(unittest.TestCase):
    """ìµœì í™” ì†ë„ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.benchmark_configs = {
            'small': PipelineConfig(
                data_length=1000,
                global_search_samples=20,
                local_refinement_steps=10,
                mc_simulations=100,
                parallel_workers=2
            ),
            'medium': PipelineConfig(
                data_length=5000,
                global_search_samples=50,
                local_refinement_steps=20,
                mc_simulations=500,
                parallel_workers=4
            ),
            'large': PipelineConfig(
                data_length=10000,
                global_search_samples=100,
                local_refinement_steps=40,
                mc_simulations=1000,
                parallel_workers=4
            )
        }
        
        self.parameter_space = {
            'target_r': (2.0, 4.0),
            'stop_atr_mult': (0.05, 0.2),
            'swing_len': (3, 10),
            'rr_percentile': (0.1, 0.4)
        }
    
    def test_small_scale_optimization_speed(self):
        """ì†Œê·œëª¨ ìµœì í™” ì†ë„ í…ŒìŠ¤íŠ¸"""
        print("âš¡ ì†Œê·œëª¨ ìµœì í™” ì†ë„ ë²¤ì¹˜ë§ˆí¬...")
        
        config = self.benchmark_configs['small']
        pipeline = OptimizationPipeline(config)
        
        # ì†ë„ ì¸¡ì •
        start_time = time.time()
        result = pipeline.run_pipeline(self.parameter_space)
        end_time = time.time()
        
        execution_time = end_time - start_time
        
        # ì„±ëŠ¥ ê¸°ì¤€ (ì†Œê·œëª¨: 30ì´ˆ ì´ë‚´)
        target_time = 30.0
        
        print(f"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ (ëª©í‘œ: {target_time}ì´ˆ)")
        print(f"   ğŸ“Š ë°ì´í„° í¬ê¸°: {config.data_length:,}ê°œ")
        print(f"   ğŸ” íƒìƒ‰ ìƒ˜í”Œ: {config.global_search_samples}ê°œ")
        
        # ì„±ëŠ¥ ê²€ì¦
        self.assertLess(execution_time, target_time * 1.5)  # 50% ì—¬ìœ 
        
        # ì²˜ë¦¬ ì†ë„ ê³„ì‚° (ìƒ˜í”Œ/ì´ˆ)
        processing_speed = config.global_search_samples / execution_time
        print(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {processing_speed:.1f} ìƒ˜í”Œ/ì´ˆ")
        
        # ìµœì†Œ ì²˜ë¦¬ ì†ë„ ê²€ì¦
        self.assertGreater(processing_speed, 0.5)  # ìµœì†Œ 0.5 ìƒ˜í”Œ/ì´ˆ
    
    def test_medium_scale_optimization_speed(self):
        """ì¤‘ê·œëª¨ ìµœì í™” ì†ë„ í…ŒìŠ¤íŠ¸"""
        print("âš¡ ì¤‘ê·œëª¨ ìµœì í™” ì†ë„ ë²¤ì¹˜ë§ˆí¬...")
        
        config = self.benchmark_configs['medium']
        pipeline = OptimizationPipeline(config)
        
        # ì†ë„ ì¸¡ì •
        start_time = time.time()
        result = pipeline.run_pipeline(self.parameter_space)
        end_time = time.time()
        
        execution_time = end_time - start_time
        
        # ì„±ëŠ¥ ê¸°ì¤€ (ì¤‘ê·œëª¨: 2ë¶„ ì´ë‚´)
        target_time = 120.0
        
        print(f"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ (ëª©í‘œ: {target_time}ì´ˆ)")
        print(f"   ğŸ“Š ë°ì´í„° í¬ê¸°: {config.data_length:,}ê°œ")
        print(f"   ğŸ” íƒìƒ‰ ìƒ˜í”Œ: {config.global_search_samples}ê°œ")
        
        # ì„±ëŠ¥ ê²€ì¦ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        self.assertLess(execution_time, target_time * 2.0)  # 100% ì—¬ìœ 
        
        # ì²˜ë¦¬ ì†ë„ ê³„ì‚°
        processing_speed = config.global_search_samples / execution_time
        print(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {processing_speed:.1f} ìƒ˜í”Œ/ì´ˆ")
    
    def test_parallel_processing_efficiency(self):
        """ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        print("ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸...")
        
        # ë‹¨ì¼ ìŠ¤ë ˆë“œ vs ë©€í‹° ìŠ¤ë ˆë“œ ë¹„êµ
        base_config = self.benchmark_configs['small']
        
        # ë‹¨ì¼ ìŠ¤ë ˆë“œ ì„¤ì •
        single_config = PipelineConfig(
            data_length=base_config.data_length,
            global_search_samples=base_config.global_search_samples,
            local_refinement_steps=base_config.local_refinement_steps,
            mc_simulations=base_config.mc_simulations,
            parallel_workers=1
        )
        
        # ë©€í‹° ìŠ¤ë ˆë“œ ì„¤ì •
        multi_config = PipelineConfig(
            data_length=base_config.data_length,
            global_search_samples=base_config.global_search_samples,
            local_refinement_steps=base_config.local_refinement_steps,
            mc_simulations=base_config.mc_simulations,
            parallel_workers=4
        )
        
        # ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‹¤í–‰
        single_pipeline = OptimizationPipeline(single_config)
        start_time = time.time()
        single_result = single_pipeline.run_pipeline(self.parameter_space)
        single_time = time.time() - start_time
        
        # ë©€í‹° ìŠ¤ë ˆë“œ ì‹¤í–‰
        multi_pipeline = OptimizationPipeline(multi_config)
        start_time = time.time()
        multi_result = multi_pipeline.run_pipeline(self.parameter_space)
        multi_time = time.time() - start_time
        
        # íš¨ìœ¨ì„± ê³„ì‚°
        speedup = single_time / multi_time if multi_time > 0 else 1.0
        efficiency = speedup / multi_config.parallel_workers
        
        print(f"   ğŸ”§ ë‹¨ì¼ ìŠ¤ë ˆë“œ: {single_time:.1f}ì´ˆ")
        print(f"   âš¡ ë©€í‹° ìŠ¤ë ˆë“œ: {multi_time:.1f}ì´ˆ ({multi_config.parallel_workers}ê°œ ì›Œì»¤)")
        print(f"   ğŸ“ˆ ì†ë„ í–¥ìƒ: {speedup:.2f}x")
        print(f"   ğŸ“Š íš¨ìœ¨ì„±: {efficiency*100:.1f}%")
        
        # ë³‘ë ¬ ì²˜ë¦¬ê°€ ë„ì›€ì´ ë˜ì–´ì•¼ í•¨ (ìµœì†Œ 20% í–¥ìƒ)
        self.assertGreater(speedup, 1.2)
        
        # íš¨ìœ¨ì„±ì´ ë„ˆë¬´ ë‚®ì§€ ì•Šì•„ì•¼ í•¨ (ìµœì†Œ 30%)
        self.assertGreater(efficiency, 0.3)
    
    def test_scalability_analysis(self):
        """í™•ì¥ì„± ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        print("ğŸ“ˆ í™•ì¥ì„± ë¶„ì„ í…ŒìŠ¤íŠ¸...")
        
        # ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸°ì—ì„œ ì„±ëŠ¥ ì¸¡ì •
        data_sizes = [500, 1000, 2000, 5000]
        performance_results = []
        
        for data_size in data_sizes:
            config = PipelineConfig(
                data_length=data_size,
                global_search_samples=20,  # ê³ ì •
                local_refinement_steps=10,  # ê³ ì •
                mc_simulations=100,  # ê³ ì •
                parallel_workers=2
            )
            
            pipeline = OptimizationPipeline(config)
            
            # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            result = pipeline.run_pipeline(self.parameter_space)
            execution_time = time.time() - start_time
            
            # ë°ì´í„° í¬ì¸íŠ¸ë‹¹ ì²˜ë¦¬ ì‹œê°„
            time_per_datapoint = execution_time / data_size
            
            performance_results.append({
                'data_size': data_size,
                'execution_time': execution_time,
                'time_per_datapoint': time_per_datapoint
            })
            
            print(f"   ğŸ“Š ë°ì´í„° í¬ê¸° {data_size:,}: {execution_time:.1f}ì´ˆ "
                  f"({time_per_datapoint*1000:.2f}ms/í¬ì¸íŠ¸)")
        
        # í™•ì¥ì„± ë¶„ì„
        # ì‹œê°„ ë³µì¡ë„ê°€ ì„ í˜•ì— ê°€ê¹Œì›Œì•¼ í•¨
        if len(performance_results) >= 2:
            first_result = performance_results[0]
            last_result = performance_results[-1]
            
            size_ratio = last_result['data_size'] / first_result['data_size']
            time_ratio = last_result['execution_time'] / first_result['execution_time']
            
            complexity_factor = time_ratio / size_ratio
            
            print(f"   ğŸ“ˆ í¬ê¸° ë¹„ìœ¨: {size_ratio:.1f}x")
            print(f"   â±ï¸ ì‹œê°„ ë¹„ìœ¨: {time_ratio:.1f}x")
            print(f"   ğŸ” ë³µì¡ë„ ê³„ìˆ˜: {complexity_factor:.2f}")
            
            # ë³µì¡ë„ê°€ ë„ˆë¬´ ë†’ì§€ ì•Šì•„ì•¼ í•¨ (3ë°° ì´í•˜)
            self.assertLess(complexity_factor, 3.0)

class TestMemoryUsageProfiling(unittest.TestCase):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.performance_optimizer = PerformanceOptimizer(
            PerformanceConfig(max_memory_usage_gb=2.0)
        )
        self.performance_optimizer.start_optimization()
    
    def tearDown(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        self.performance_optimizer.stop_optimization()
    
    def test_memory_usage_during_optimization(self):
        """ìµœì í™” ì¤‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§  ìµœì í™” ì¤‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§...")
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
        initial_memory = psutil.virtual_memory()
        process = psutil.Process()
        initial_process_memory = process.memory_info().rss / (1024**3)  # GB
        
        print(f"   ğŸ“Š ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {initial_memory.available/(1024**3):.1f}GB ì‚¬ìš© ê°€ëŠ¥")
        print(f"   ğŸ” ì´ˆê¸° í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {initial_process_memory:.2f}GB")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        memory_samples = []
        
        def memory_monitor():
            for _ in range(20):  # 20ì´ˆê°„ ëª¨ë‹ˆí„°ë§
                current_memory = psutil.virtual_memory()
                current_process_memory = process.memory_info().rss / (1024**3)
                
                memory_samples.append({
                    'timestamp': time.time(),
                    'system_available_gb': current_memory.available / (1024**3),
                    'process_memory_gb': current_process_memory,
                    'system_usage_pct': current_memory.percent
                })
                
                time.sleep(1)
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
        monitor_thread = threading.Thread(target=memory_monitor, daemon=True)
        monitor_thread.start()
        
        # ìµœì í™” ì‹¤í–‰
        config = PipelineConfig(
            data_length=5000,
            global_search_samples=30,
            mc_simulations=200,
            parallel_workers=2
        )
        
        pipeline = OptimizationPipeline(config)
        parameter_space = {
            'target_r': (2.0, 4.0),
            'stop_atr_mult': (0.05, 0.2)
        }
        
        result = pipeline.run_pipeline(parameter_space)
        
        # ëª¨ë‹ˆí„°ë§ ì™„ë£Œ ëŒ€ê¸°
        monitor_thread.join(timeout=25)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        if memory_samples:
            max_process_memory = max(sample['process_memory_gb'] for sample in memory_samples)
            avg_process_memory = np.mean([sample['process_memory_gb'] for sample in memory_samples])
            memory_growth = max_process_memory - initial_process_memory
            
            print(f"   ğŸ“ˆ ìµœëŒ€ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {max_process_memory:.2f}GB")
            print(f"   ğŸ“Š í‰ê·  í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {avg_process_memory:.2f}GB")
            print(f"   ğŸ“ ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {memory_growth:.2f}GB")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²€ì¦
            self.assertLess(max_process_memory, 4.0)  # 4GB ì´í•˜
            self.assertLess(memory_growth, 2.0)       # ì¦ê°€ëŸ‰ 2GB ì´í•˜
            
            # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì²´í¬ (ë§ˆì§€ë§‰ 10% ìƒ˜í”Œì˜ í‰ê· ì´ ì²˜ìŒ 10%ë³´ë‹¤ í¬ê²Œ ë†’ì§€ ì•Šì•„ì•¼ í•¨)
            if len(memory_samples) >= 10:
                early_samples = memory_samples[:len(memory_samples)//10]
                late_samples = memory_samples[-len(memory_samples)//10:]
                
                early_avg = np.mean([s['process_memory_gb'] for s in early_samples])
                late_avg = np.mean([s['process_memory_gb'] for s in late_samples])
                
                memory_leak_ratio = late_avg / early_avg if early_avg > 0 else 1.0
                
                print(f"   ğŸ” ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¹„ìœ¨: {memory_leak_ratio:.2f}")
                
                # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ê°€ ì‹¬í•˜ì§€ ì•Šì•„ì•¼ í•¨ (50% ì´í•˜ ì¦ê°€)
                self.assertLess(memory_leak_ratio, 1.5)
    
    def test_cache_memory_management(self):
        """ìºì‹œ ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("ğŸ’¾ ìºì‹œ ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸...")
        
        memory_manager = self.performance_optimizer.memory_manager
        
        # ì´ˆê¸° ìºì‹œ ìƒíƒœ
        initial_cache_stats = memory_manager.get_cache_stats()
        print(f"   ğŸ“Š ì´ˆê¸° ìºì‹œ: {initial_cache_stats['cache_entries']}ê°œ í•­ëª©")
        
        # ëŒ€ëŸ‰ ë°ì´í„° ìºì‹œ
        large_data_items = []
        
        for i in range(50):
            # í° ë°ì´í„° ìƒì„± (ê°ê° ~10MB)
            large_data = np.random.random((1000, 1000))
            cache_key = f"large_data_{i}"
            
            memory_manager.cache_set(cache_key, large_data)
            large_data_items.append(cache_key)
            
            # ìºì‹œ ìƒíƒœ í™•ì¸
            cache_stats = memory_manager.get_cache_stats()
            
            if i % 10 == 0:
                print(f"   ğŸ“ˆ {i+1}ê°œ í•­ëª© í›„: {cache_stats['cache_size_mb']:.1f}MB")
        
        # ìµœì¢… ìºì‹œ ìƒíƒœ
        final_cache_stats = memory_manager.get_cache_stats()
        
        print(f"   ğŸ“Š ìµœì¢… ìºì‹œ: {final_cache_stats['cache_entries']}ê°œ í•­ëª©")
        print(f"   ğŸ’¾ ìµœì¢… í¬ê¸°: {final_cache_stats['cache_size_mb']:.1f}MB")
        
        # ìºì‹œ í¬ê¸° ì œí•œ ê²€ì¦
        max_cache_size = self.performance_optimizer.config.cache_size_mb
        self.assertLessEqual(final_cache_stats['cache_size_mb'], max_cache_size * 1.2)  # 20% ì—¬ìœ 
        
        # ìºì‹œ ì •ë¦¬ í…ŒìŠ¤íŠ¸
        memory_manager.cleanup_memory()
        
        after_cleanup_stats = memory_manager.get_cache_stats()
        print(f"   ğŸ§¹ ì •ë¦¬ í›„: {after_cleanup_stats['cache_entries']}ê°œ í•­ëª©, "
              f"{after_cleanup_stats['cache_size_mb']:.1f}MB")
        
        # ì •ë¦¬ í›„ í¬ê¸°ê°€ ì¤„ì–´ë“¤ì–´ì•¼ í•¨
        self.assertLessEqual(after_cleanup_stats['cache_size_mb'], 
                           final_cache_stats['cache_size_mb'])
    
    def test_garbage_collection_efficiency(self):
        """ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        print("ğŸ—‘ï¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸...")
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
        gc.collect()  # ì´ˆê¸° ì •ë¦¬
        initial_objects = len(gc.get_objects())
        
        # ëŒ€ëŸ‰ ê°ì²´ ìƒì„±
        temp_objects = []
        
        for i in range(1000):
            # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ê°ì²´ ìƒì„±
            obj = {
                'data': np.random.random(100),
                'metadata': {'id': i, 'timestamp': datetime.now()},
                'nested': [np.random.random(10) for _ in range(10)]
            }
            temp_objects.append(obj)
        
        after_creation_objects = len(gc.get_objects())
        
        # ê°ì²´ ì°¸ì¡° í•´ì œ
        temp_objects.clear()
        del temp_objects
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        collected_count = gc.collect()
        
        after_gc_objects = len(gc.get_objects())
        
        print(f"   ğŸ“Š ì´ˆê¸° ê°ì²´ ìˆ˜: {initial_objects:,}")
        print(f"   ğŸ“ˆ ìƒì„± í›„ ê°ì²´ ìˆ˜: {after_creation_objects:,}")
        print(f"   ğŸ—‘ï¸ ìˆ˜ì§‘ëœ ê°ì²´ ìˆ˜: {collected_count:,}")
        print(f"   ğŸ“‰ ì •ë¦¬ í›„ ê°ì²´ ìˆ˜: {after_gc_objects:,}")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ íš¨ìœ¨ì„± ê²€ì¦
        objects_created = after_creation_objects - initial_objects
        objects_remaining = after_gc_objects - initial_objects
        
        cleanup_efficiency = 1 - (objects_remaining / objects_created) if objects_created > 0 else 1
        
        print(f"   ğŸ“Š ì •ë¦¬ íš¨ìœ¨ì„±: {cleanup_efficiency*100:.1f}%")
        
        # ìµœì†Œ 50% ì´ìƒ ì •ë¦¬ë˜ì–´ì•¼ í•¨
        self.assertGreater(cleanup_efficiency, 0.5)

class TestHistoricalBacktestComparison(unittest.TestCase):
    """íˆìŠ¤í† ë¦¬ì»¬ ë°±í…ŒìŠ¤íŠ¸ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.performance_evaluator = PerformanceEvaluator()
        
        # í…ŒìŠ¤íŠ¸ìš© íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìƒì„±
        np.random.seed(42)
        self.historical_data = self._generate_historical_data()
        
        # ì•Œë ¤ì§„ ì¢‹ì€ íŒŒë¼ë¯¸í„° (ê¸°ì¤€ì )
        self.benchmark_parameters = {
            'target_r': 3.0,
            'stop_atr_mult': 0.1,
            'swing_len': 5,
            'rr_percentile': 0.25
        }
    
    def _generate_historical_data(self) -> pd.DataFrame:
        """íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìƒì„±"""
        # 3ë…„ê°„ì˜ ì‹œê°„ë³„ ë°ì´í„°
        dates = pd.date_range(start='2021-01-01', end='2023-12-31', freq='1H')
        
        # í˜„ì‹¤ì ì¸ ê°€ê²© ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜
        returns = np.random.normal(0, 0.002, len(dates))  # 0.2% ì‹œê°„ë‹¹ ë³€ë™ì„±
        
        # íŠ¸ë Œë“œì™€ ë³€ë™ì„± ì²´ì œ ì¶”ê°€
        trend_changes = np.random.choice([0, 1], size=len(dates), p=[0.99, 0.01])
        volatility_regime = np.random.choice([0.5, 1.0, 2.0], size=len(dates), p=[0.7, 0.2, 0.1])
        
        returns = returns * volatility_regime
        
        # ê°€ê²© ê³„ì‚°
        prices = 50000 * np.exp(np.cumsum(returns))
        
        return pd.DataFrame({
            'timestamp': dates,
            'open': prices * (1 + np.random.normal(0, 0.0001, len(dates))),
            'high': prices * (1 + np.abs(np.random.normal(0, 0.001, len(dates)))),
            'low': prices * (1 - np.abs(np.random.normal(0, 0.001, len(dates)))),
            'close': prices,
            'volume': np.random.uniform(100, 1000, len(dates))
        })
    
    def test_parameter_sensitivity_analysis(self):
        """íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        print("ğŸ›ï¸ íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸...")
        
        # ê¸°ì¤€ íŒŒë¼ë¯¸í„°ë¡œ ë°±í…ŒìŠ¤íŠ¸
        baseline_trades = self._simulate_backtest(self.benchmark_parameters)
        baseline_metrics = self.performance_evaluator.calculate_metrics(baseline_trades)
        baseline_score = self.performance_evaluator.calculate_composite_score(baseline_metrics)
        
        print(f"   ğŸ“Š ê¸°ì¤€ ì ìˆ˜: {baseline_score:.4f}")
        print(f"   ğŸ“ˆ ê¸°ì¤€ PF: {baseline_metrics.profit_factor:.2f}")
        
        # ê° íŒŒë¼ë¯¸í„°ë³„ ë¯¼ê°ë„ í…ŒìŠ¤íŠ¸
        sensitivity_results = {}
        
        for param_name, base_value in self.benchmark_parameters.items():
            param_scores = []
            
            # íŒŒë¼ë¯¸í„° ê°’ ë³€í™” (-20%, -10%, 0%, +10%, +20%)
            variations = [-0.2, -0.1, 0.0, 0.1, 0.2]
            
            for variation in variations:
                test_params = self.benchmark_parameters.copy()
                
                if isinstance(base_value, (int, float)):
                    test_params[param_name] = base_value * (1 + variation)
                
                # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                test_trades = self._simulate_backtest(test_params)
                test_metrics = self.performance_evaluator.calculate_metrics(test_trades)
                test_score = self.performance_evaluator.calculate_composite_score(test_metrics)
                
                param_scores.append(test_score)
            
            # ë¯¼ê°ë„ ê³„ì‚° (ì ìˆ˜ ë³€í™”ì˜ í‘œì¤€í¸ì°¨)
            sensitivity = np.std(param_scores)
            sensitivity_results[param_name] = {
                'sensitivity': sensitivity,
                'scores': param_scores,
                'variations': variations
            }
            
            print(f"   ğŸ›ï¸ {param_name} ë¯¼ê°ë„: {sensitivity:.4f}")
        
        # ê°€ì¥ ë¯¼ê°í•œ íŒŒë¼ë¯¸í„° ì‹ë³„
        most_sensitive = max(sensitivity_results.keys(), 
                           key=lambda k: sensitivity_results[k]['sensitivity'])
        
        print(f"   ğŸ” ê°€ì¥ ë¯¼ê°í•œ íŒŒë¼ë¯¸í„°: {most_sensitive}")
        
        # ë¯¼ê°ë„ê°€ í•©ë¦¬ì  ë²”ìœ„ì— ìˆëŠ”ì§€ í™•ì¸
        for param_name, result in sensitivity_results.items():
            self.assertGreater(result['sensitivity'], 0.001)  # ìµœì†Œ ë¯¼ê°ë„
            self.assertLess(result['sensitivity'], 1.0)       # ìµœëŒ€ ë¯¼ê°ë„
    
    def test_regime_performance_consistency(self):
        """ë ˆì§ë³„ ì„±ëŠ¥ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        print("ğŸ“Š ë ˆì§ë³„ ì„±ëŠ¥ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸...")
        
        # ë°ì´í„°ë¥¼ ë³€ë™ì„± ë ˆì§ë³„ë¡œ ë¶„í• 
        returns = self.historical_data['close'].pct_change().dropna()
        
        # ë¡¤ë§ ë³€ë™ì„± ê³„ì‚° (30ì¼ ìœˆë„ìš°)
        rolling_vol = returns.rolling(30*24).std()  # 30ì¼ * 24ì‹œê°„
        
        # ë³€ë™ì„± ë ˆì§ ë¶„ë¥˜
        vol_quantiles = rolling_vol.quantile([0.33, 0.67])
        
        low_vol_mask = rolling_vol <= vol_quantiles.iloc[0]
        high_vol_mask = rolling_vol >= vol_quantiles.iloc[1]
        medium_vol_mask = ~(low_vol_mask | high_vol_mask)
        
        regimes = {
            'low_volatility': self.historical_data[low_vol_mask],
            'medium_volatility': self.historical_data[medium_vol_mask],
            'high_volatility': self.historical_data[high_vol_mask]
        }
        
        regime_results = {}
        
        for regime_name, regime_data in regimes.items():
            if len(regime_data) < 100:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰
                continue
            
            # ë ˆì§ë³„ ë°±í…ŒìŠ¤íŠ¸
            regime_trades = self._simulate_backtest_on_data(
                self.benchmark_parameters, regime_data
            )
            
            if regime_trades:
                regime_metrics = self.performance_evaluator.calculate_metrics(regime_trades)
                regime_score = self.performance_evaluator.calculate_composite_score(regime_metrics)
                
                regime_results[regime_name] = {
                    'score': regime_score,
                    'profit_factor': regime_metrics.profit_factor,
                    'win_rate': regime_metrics.win_rate,
                    'trade_count': len(regime_trades)
                }
                
                print(f"   ğŸ“Š {regime_name}: ì ìˆ˜={regime_score:.4f}, "
                      f"PF={regime_metrics.profit_factor:.2f}, "
                      f"ê±°ë˜ìˆ˜={len(regime_trades)}")
        
        # ë ˆì§ ê°„ ì„±ëŠ¥ ì¼ê´€ì„± ê²€ì¦
        if len(regime_results) >= 2:
            scores = [result['score'] for result in regime_results.values()]
            score_std = np.std(scores)
            score_mean = np.mean(scores)
            
            consistency_ratio = 1 - (score_std / abs(score_mean)) if score_mean != 0 else 0
            
            print(f"   ğŸ“ˆ ë ˆì§ ê°„ ì¼ê´€ì„±: {consistency_ratio*100:.1f}%")
            
            # ìµœì†Œ ì¼ê´€ì„± ìš”êµ¬ (50% ì´ìƒ)
            self.assertGreater(consistency_ratio, 0.5)
    
    def test_out_of_sample_degradation(self):
        """í‘œë³¸ ì™¸ ì„±ëŠ¥ ì €í•˜ í…ŒìŠ¤íŠ¸"""
        print("ğŸ“‰ í‘œë³¸ ì™¸ ì„±ëŠ¥ ì €í•˜ í…ŒìŠ¤íŠ¸...")
        
        # ë°ì´í„°ë¥¼ í›ˆë ¨/í…ŒìŠ¤íŠ¸ë¡œ ë¶„í•  (70%/30%)
        split_point = int(len(self.historical_data) * 0.7)
        
        train_data = self.historical_data[:split_point]
        test_data = self.historical_data[split_point:]
        
        # í›ˆë ¨ ë°ì´í„°ì—ì„œ ë°±í…ŒìŠ¤íŠ¸
        train_trades = self._simulate_backtest_on_data(self.benchmark_parameters, train_data)
        train_metrics = self.performance_evaluator.calculate_metrics(train_trades)
        train_score = self.performance_evaluator.calculate_composite_score(train_metrics)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë°±í…ŒìŠ¤íŠ¸
        test_trades = self._simulate_backtest_on_data(self.benchmark_parameters, test_data)
        test_metrics = self.performance_evaluator.calculate_metrics(test_trades)
        test_score = self.performance_evaluator.calculate_composite_score(test_metrics)
        
        # ì„±ëŠ¥ ì €í•˜ ê³„ì‚°
        performance_degradation = (train_score - test_score) / abs(train_score) if train_score != 0 else 0
        
        print(f"   ğŸ“Š í›ˆë ¨ ì ìˆ˜: {train_score:.4f}")
        print(f"   ğŸ“‰ í…ŒìŠ¤íŠ¸ ì ìˆ˜: {test_score:.4f}")
        print(f"   ğŸ“ˆ ì„±ëŠ¥ ì €í•˜: {performance_degradation*100:.1f}%")
        
        # ì„±ëŠ¥ ì €í•˜ê°€ ë„ˆë¬´ í¬ì§€ ì•Šì•„ì•¼ í•¨ (50% ì´í•˜)
        self.assertLess(performance_degradation, 0.5)
        
        # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì´ ìµœì†Œ ê¸°ì¤€ì„ ë§Œì¡±í•´ì•¼ í•¨
        self.assertGreater(test_score, -1.0)  # ìµœì†Œ ì ìˆ˜
    
    def _simulate_backtest(self, parameters: Dict) -> List[Dict]:
        """íŒŒë¼ë¯¸í„°ë¡œ ë°±í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜"""
        return self._simulate_backtest_on_data(parameters, self.historical_data)
    
    def _simulate_backtest_on_data(self, parameters: Dict, data: pd.DataFrame) -> List[Dict]:
        """íŠ¹ì • ë°ì´í„°ì—ì„œ ë°±í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜"""
        trades = []
        
        # ê°„ë‹¨í•œ ë°±í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜
        for i in range(100, len(data) - 100, 50):  # 50ë°”ë§ˆë‹¤ ê±°ë˜ ê¸°íšŒ
            
            # íŒŒë¼ë¯¸í„° ê¸°ë°˜ ê±°ë˜ ì‹ í˜¸ ìƒì„± (ë‹¨ìˆœí™”)
            target_r = parameters.get('target_r', 3.0)
            stop_atr_mult = parameters.get('stop_atr_mult', 0.1)
            
            # ê°€ìƒì˜ ê±°ë˜ ê²°ê³¼ ìƒì„±
            if np.random.random() < 0.6:  # 60% ìŠ¹ë¥ 
                pnl_pct = np.random.normal(0.02, 0.01)  # í‰ê·  2% ìˆ˜ìµ
            else:
                pnl_pct = np.random.normal(-0.01, 0.005)  # í‰ê·  1% ì†ì‹¤
            
            # íŒŒë¼ë¯¸í„° ì˜í–¥ ë°˜ì˜
            pnl_pct *= (target_r / 3.0)  # target_r ì˜í–¥
            pnl_pct *= (1 - stop_atr_mult * 5)  # stop_atr_mult ì˜í–¥
            
            trades.append({
                'entry_time': data.iloc[i]['timestamp'],
                'exit_time': data.iloc[i+10]['timestamp'],
                'pnl': pnl_pct * 10000,  # $10,000 ê¸°ì¤€
                'pnl_pct': pnl_pct,
                'quantity': 1.0,
                'side': 'long'
            })
        
        return trades

class TestRiskManagementValidation(unittest.TestCase):
    """ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.kelly_sizer = KellyPositionSizer()
        self.monitor = RealtimeMonitor(MonitoringConfig(
            daily_loss_limit_pct=0.05,
            max_consecutive_losses=5
        ))
        
        # í…ŒìŠ¤íŠ¸ìš© ê±°ë˜ ë°ì´í„°
        np.random.seed(42)
        self.test_trades = self._generate_test_trades()
    
    def _generate_test_trades(self) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ìš© ê±°ë˜ ë°ì´í„° ìƒì„±"""
        trades = []
        
        for i in range(200):
            # ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ í¬í•¨
            if i < 50:
                # ì´ˆê¸°: ì¢‹ì€ ì„±ê³¼
                win_prob = 0.7
                avg_win = 0.025
                avg_loss = 0.012
            elif i < 100:
                # ì¤‘ê°„: ë³´í†µ ì„±ê³¼
                win_prob = 0.6
                avg_win = 0.02
                avg_loss = 0.015
            elif i < 150:
                # í›„ë°˜: ë‚˜ìœ ì„±ê³¼ (DD ë°œìƒ)
                win_prob = 0.4
                avg_win = 0.015
                avg_loss = 0.02
            else:
                # íšŒë³µ: ë‹¤ì‹œ ì¢‹ì€ ì„±ê³¼
                win_prob = 0.65
                avg_win = 0.022
                avg_loss = 0.013
            
            if np.random.random() < win_prob:
                pnl_pct = np.random.normal(avg_win, avg_win * 0.3)
            else:
                pnl_pct = np.random.normal(-avg_loss, avg_loss * 0.3)
            
            trades.append({
                'timestamp': datetime.now() - timedelta(days=200-i),
                'pnl_pct': pnl_pct,
                'win': pnl_pct > 0
            })
        
        return trades
    
    def test_position_sizing_risk_limits(self):
        """í¬ì§€ì…˜ ì‚¬ì´ì§• ë¦¬ìŠ¤í¬ í•œë„ í…ŒìŠ¤íŠ¸"""
        print("âš–ï¸ í¬ì§€ì…˜ ì‚¬ì´ì§• ë¦¬ìŠ¤í¬ í•œë„ í…ŒìŠ¤íŠ¸...")
        
        # ë‹¤ì–‘í•œ ê³„ì¢Œ í¬ê¸°ì—ì„œ ë¦¬ìŠ¤í¬ í•œë„ ê²€ì¦
        test_balances = [500, 1000, 5000, 10000, 25000, 50000]
        
        for balance in test_balances:
            recommendation = self.kelly_sizer.get_position_recommendation(
                balance, self.test_trades, current_dd=0.0
            )
            
            # ë¦¬ìŠ¤í¬ ë¹„ìœ¨ ê³„ì‚°
            risk_ratio = recommendation['risk_amount'] / balance
            
            print(f"   ğŸ’° ì”ê³  ${balance:,}: í¬ì§€ì…˜ ${recommendation['position_size']:.2f}, "
                  f"ë¦¬ìŠ¤í¬ {risk_ratio*100:.1f}%")
            
            # ë¦¬ìŠ¤í¬ í•œë„ ê²€ì¦ (ìµœëŒ€ 5%)
            self.assertLessEqual(risk_ratio, 0.05)
            
            # ìµœì†Œ ì£¼ë¬¸ ê¸ˆì•¡ ê²€ì¦
            self.assertGreaterEqual(recommendation['position_size'], 20.0)
            
            # í¬ì§€ì…˜ì´ ê³„ì¢Œ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ì§€ ì•ŠìŒ
            self.assertLessEqual(recommendation['position_size'], balance)
    
    def test_drawdown_scaling_effectiveness(self):
        """ë“œë¡œìš°ë‹¤ìš´ ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ì„± í…ŒìŠ¤íŠ¸"""
        print("ğŸ“‰ ë“œë¡œìš°ë‹¤ìš´ ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ì„± í…ŒìŠ¤íŠ¸...")
        
        balance = 10000.0
        dd_levels = [0.0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]
        
        position_sizes = []
        
        for dd_level in dd_levels:
            recommendation = self.kelly_sizer.get_position_recommendation(
                balance, self.test_trades, current_dd=dd_level
            )
            
            position_sizes.append(recommendation['position_size'])
            
            print(f"   ğŸ“Š DD {dd_level*100:3.0f}%: í¬ì§€ì…˜ ${recommendation['position_size']:6.2f}")
        
        # DDê°€ ì¦ê°€í• ìˆ˜ë¡ í¬ì§€ì…˜ì´ ê°ì†Œí•´ì•¼ í•¨
        for i in range(1, len(position_sizes)):
            self.assertLessEqual(position_sizes[i], position_sizes[i-1] * 1.01)  # 1% ì˜¤ì°¨ í—ˆìš©
        
        # ìµœëŒ€ DDì—ì„œë„ ìµœì†Œ í¬ì§€ì…˜ì€ ìœ ì§€
        self.assertGreaterEqual(position_sizes[-1], 20.0)  # ìµœì†Œ ì£¼ë¬¸ ê¸ˆì•¡
        
        # ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ ê³„ì‚°
        max_scaling = (position_sizes[0] - position_sizes[-1]) / position_sizes[0]
        print(f"   ğŸ“ˆ ìµœëŒ€ ìŠ¤ì¼€ì¼ë§: {max_scaling*100:.1f}%")
        
        # ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§ì´ ì ìš©ë˜ì–´ì•¼ í•¨ (20% ì´ìƒ)
        self.assertGreater(max_scaling, 0.2)
    
    def test_consecutive_loss_protection(self):
        """ì—°ì† ì†ì‹¤ ë³´í˜¸ í…ŒìŠ¤íŠ¸"""
        print("ğŸ›¡ï¸ ì—°ì† ì†ì‹¤ ë³´í˜¸ í…ŒìŠ¤íŠ¸...")
        
        self.monitor.start_monitoring(10000.0)
        
        # ì—°ì† ì†ì‹¤ ì‹œë®¬ë ˆì´ì…˜
        consecutive_losses = 0
        max_consecutive = 0
        
        for i, trade in enumerate(self.test_trades):
            if not trade['win']:
                consecutive_losses += 1
                max_consecutive = max(max_consecutive, consecutive_losses)
            else:
                consecutive_losses = 0
            
            # ê±°ë˜ ê¸°ë¡
            from realtime_monitoring_system import TradeEvent
            
            trade_event = TradeEvent(
                timestamp=trade['timestamp'],
                symbol='BTCUSDT',
                side='long',
                quantity=0.1,
                price=50000.0,
                pnl=trade['pnl_pct'] * 10000
            )
            
            self.monitor.record_trade(trade_event)
            
            # ìƒíƒœ í™•ì¸
            status = self.monitor.get_monitoring_status()
            
            # ì—°ì† ì†ì‹¤ í•œë„ í™•ì¸
            if status['consecutive_losses'] >= 5:  # ì„¤ì •ëœ í•œë„
                print(f"   ğŸš¨ ì—°ì† ì†ì‹¤ í•œë„ ë„ë‹¬: {status['consecutive_losses']}íšŒ")
                break
        
        print(f"   ğŸ“Š ìµœëŒ€ ì—°ì† ì†ì‹¤: {max_consecutive}íšŒ")
        print(f"   ğŸ›¡ï¸ ëª¨ë‹ˆí„°ë§ ìƒíƒœ: {self.monitor.trading_state.value}")
        
        # ì—°ì† ì†ì‹¤ ë³´í˜¸ê°€ ì‘ë™í•´ì•¼ í•¨
        final_status = self.monitor.get_monitoring_status()
        if max_consecutive >= 5:
            self.assertNotEqual(final_status['trading_state'], 'active')
        
        self.monitor.stop_monitoring()
    
    def test_daily_loss_limit_enforcement(self):
        """ì¼ì¼ ì†ì‹¤ í•œë„ ì‹œí–‰ í…ŒìŠ¤íŠ¸"""
        print("ğŸ“… ì¼ì¼ ì†ì‹¤ í•œë„ ì‹œí–‰ í…ŒìŠ¤íŠ¸...")
        
        initial_balance = 10000.0
        daily_loss_limit = 0.05  # 5%
        
        self.monitor.start_monitoring(initial_balance)
        
        # í° ì†ì‹¤ ì‹œë®¬ë ˆì´ì…˜
        cumulative_loss = 0.0
        
        for i in range(20):
            # ê°ê° 1% ì†ì‹¤
            loss_amount = initial_balance * 0.01
            cumulative_loss += loss_amount
            
            new_balance = initial_balance - cumulative_loss
            self.monitor.update_balance(new_balance)
            
            status = self.monitor.get_monitoring_status()
            daily_loss_pct = abs(status['daily_pnl_pct'])
            
            print(f"   ğŸ“‰ ì†ì‹¤ {i+1}: {daily_loss_pct*100:.1f}% "
                  f"(ìƒíƒœ: {status['trading_state']})")
            
            # ì†ì‹¤ í•œë„ ë„ë‹¬ ì‹œ ê±°ë˜ ì¤‘ì§€ í™•ì¸
            if daily_loss_pct >= daily_loss_limit:
                self.assertEqual(status['trading_state'], 'stopped')
                print(f"   ğŸ›‘ ì¼ì¼ ì†ì‹¤ í•œë„ ë„ë‹¬ë¡œ ê±°ë˜ ì¤‘ì§€")
                break
        
        self.monitor.stop_monitoring()
    
    def test_risk_metrics_calculation_accuracy(self):
        """ë¦¬ìŠ¤í¬ ì§€í‘œ ê³„ì‚° ì •í™•ì„± í…ŒìŠ¤íŠ¸"""
        print("ğŸ“Š ë¦¬ìŠ¤í¬ ì§€í‘œ ê³„ì‚° ì •í™•ì„± í…ŒìŠ¤íŠ¸...")
        
        # ì•Œë ¤ì§„ ê²°ê³¼ë¥¼ ê°€ì§„ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        known_trades = [
            {'pnl_pct': 0.02, 'win': True},   # 2% ìˆ˜ìµ
            {'pnl_pct': -0.01, 'win': False}, # 1% ì†ì‹¤
            {'pnl_pct': 0.03, 'win': True},   # 3% ìˆ˜ìµ
            {'pnl_pct': -0.015, 'win': False}, # 1.5% ì†ì‹¤
            {'pnl_pct': 0.025, 'win': True},  # 2.5% ìˆ˜ìµ
        ]
        
        # ì˜ˆìƒ ê²°ê³¼ ê³„ì‚°
        expected_win_rate = 3/5  # 60%
        expected_avg_win = (0.02 + 0.03 + 0.025) / 3  # 2.5%
        expected_avg_loss = (0.01 + 0.015) / 2  # 1.25%
        expected_profit_factor = (0.02 + 0.03 + 0.025) / (0.01 + 0.015)  # 3.0
        
        # ì¼ˆë¦¬ ê³„ì‚°
        trade_stats = self.kelly_sizer.calculate_trade_statistics(known_trades)
        
        print(f"   ğŸ“Š ê³„ì‚°ëœ ìŠ¹ë¥ : {trade_stats.win_rate:.3f} (ì˜ˆìƒ: {expected_win_rate:.3f})")
        print(f"   ğŸ“ˆ ê³„ì‚°ëœ í‰ê·  ìˆ˜ìµ: {trade_stats.avg_win:.4f} (ì˜ˆìƒ: {expected_avg_win:.4f})")
        print(f"   ğŸ“‰ ê³„ì‚°ëœ í‰ê·  ì†ì‹¤: {trade_stats.avg_loss:.4f} (ì˜ˆìƒ: {expected_avg_loss:.4f})")
        print(f"   ğŸ’° ê³„ì‚°ëœ PF: {trade_stats.profit_factor:.2f} (ì˜ˆìƒ: {expected_profit_factor:.2f})")
        
        # ì •í™•ì„± ê²€ì¦ (1% ì˜¤ì°¨ í—ˆìš©)
        self.assertAlmostEqual(trade_stats.win_rate, expected_win_rate, delta=0.01)
        self.assertAlmostEqual(trade_stats.avg_win, expected_avg_win, delta=0.001)
        self.assertAlmostEqual(trade_stats.avg_loss, expected_avg_loss, delta=0.001)
        self.assertAlmostEqual(trade_stats.profit_factor, expected_profit_factor, delta=0.1)

class TestPerformanceValidationSuite:
    """ì„±ëŠ¥ ë° ê²€ì¦ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™”"""
        self.test_classes = [
            TestOptimizationSpeedBenchmark,
            TestMemoryUsageProfiling,
            TestHistoricalBacktestComparison,
            TestRiskManagementValidation
        ]
    
    def run_all_performance_tests(self):
        """ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("âš¡ ì„±ëŠ¥ ë° ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘")
        print("="*80)
        
        total_tests = 0
        passed_tests = 0
        failed_tests = 0
        
        for test_class in self.test_classes:
            print(f"\nğŸ“‹ {test_class.__name__} ì‹¤í–‰:")
            print("-" * 60)
            
            # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„± ë° ì‹¤í–‰
            suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
            runner = unittest.TextTestRunner(verbosity=0, stream=open(os.devnull, 'w'))
            result = runner.run(suite)
            
            # ê²°ê³¼ ì§‘ê³„
            class_total = result.testsRun
            class_failed = len(result.failures) + len(result.errors)
            class_passed = class_total - class_failed
            
            total_tests += class_total
            passed_tests += class_passed
            failed_tests += class_failed
            
            print(f"   ì‹¤í–‰: {class_total}ê°œ, í†µê³¼: {class_passed}ê°œ, ì‹¤íŒ¨: {class_failed}ê°œ")
            
            # ì‹¤íŒ¨ ìƒì„¸ ì •ë³´
            if result.failures or result.errors:
                print(f"   âŒ ë¬¸ì œ ë°œìƒ:")
                for test, traceback in result.failures + result.errors:
                    error_lines = traceback.strip().split('\n')
                    error_msg = error_lines[-1] if error_lines else "Unknown error"
                    print(f"      - {test}: {error_msg}")
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì „ì²´ ê²°ê³¼:")
        print("="*80)
        print(f"   ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        print(f"   í†µê³¼: {passed_tests}ê°œ ({passed_tests/total_tests*100:.1f}%)")
        print(f"   ì‹¤íŒ¨: {failed_tests}ê°œ ({failed_tests/total_tests*100:.1f}%)")
        
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        if success_rate >= 0.9:
            print(f"   ğŸ‰ ì„±ëŠ¥ í’ˆì§ˆ: ìš°ìˆ˜ ({success_rate*100:.1f}%)")
        elif success_rate >= 0.8:
            print(f"   âœ… ì„±ëŠ¥ í’ˆì§ˆ: ì–‘í˜¸ ({success_rate*100:.1f}%)")
        elif success_rate >= 0.7:
            print(f"   âš ï¸ ì„±ëŠ¥ í’ˆì§ˆ: ë³´í†µ ({success_rate*100:.1f}%)")
        else:
            print(f"   âŒ ì„±ëŠ¥ í’ˆì§ˆ: ê°œì„  í•„ìš” ({success_rate*100:.1f}%)")
        
        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': failed_tests,
            'success_rate': success_rate
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì„±ëŠ¥ ë° ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì‹¤í–‰")
    print("="*80)
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰
    test_suite = TestPerformanceValidationSuite()
    results = test_suite.run_all_performance_tests()
    
    print(f"\nğŸ¯ í•µì‹¬ íŠ¹ì§•:")
    print(f"   â€¢ ìµœì í™” ì†ë„ ë²¤ì¹˜ë§ˆí¬ ë° í™•ì¥ì„± ë¶„ì„")
    print(f"   â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§ ë° ëˆ„ìˆ˜ ê°ì§€")
    print(f"   â€¢ íˆìŠ¤í† ë¦¬ì»¬ ë°±í…ŒìŠ¤íŠ¸ ë¹„êµ ë° ë¯¼ê°ë„ ë¶„ì„")
    print(f"   â€¢ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ê²€ì¦")
    print(f"   â€¢ ì„±ëŠ¥ ì €í•˜ ë° ë ˆì§ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸")
    print(f"   â€¢ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ íš¨ê³¼ì„± ê²€ì¦")
    
    return results

if __name__ == "__main__":
    main()