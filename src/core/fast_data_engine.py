#!/usr/bin/env python3
"""
ê³ ì† ë°ì´í„° ì—”ì§„
- Parquet columnar storage with float32 downcasting
- Pre-computed indicators cached as ndarray
- Event-driven backtesting with incremental state updates
- Numpy vectorization + Numba JIT compilation
"""

import os
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
from numba import njit, prange
import psutil
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings("ignore")

# Ray for parallel processing (Windowsì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ)
RAY_AVAILABLE = False

# Joblib as fallback
try:
    from joblib import Parallel, delayed

    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False
    print("âš ï¸ Joblib not available, using single-threaded processing")


class FastDataEngine:
    def __init__(self, cache_dir: str = "data_cache"):
        """ê³ ì† ë°ì´í„° ì—”ì§„ ì´ˆê¸°í™”"""
        self.cache_dir = cache_dir
        self.setup_environment()
        self.setup_cache_directory()

        # ë©”ëª¨ë¦¬ ë° CPU ì„¤ì •
        self.setup_resource_limits()

        # ìºì‹œëœ ë°ì´í„°
        self.cached_data = {}
        self.cached_indicators = {}

        print("ğŸš€ ê³ ì† ë°ì´í„° ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ìºì‹œ ë””ë ‰í† ë¦¬: {self.cache_dir}")
        print(f"   CPU ì½”ì–´: {self.max_workers}ê°œ")
        print(f"   ë©”ëª¨ë¦¬ ì œí•œ: {self.max_memory_gb:.1f}GB")

    def setup_environment(self):
        """í™˜ê²½ ì„¤ì • - MKL ìŠ¤ë ˆë”© ì œì–´"""
        os.environ["MKL_NUM_THREADS"] = "1"
        os.environ["NUMEXPR_NUM_THREADS"] = "1"
        os.environ["OMP_NUM_THREADS"] = "1"

        # RayëŠ” Windowsì—ì„œ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ joblib ì‚¬ìš©
        print("âœ… Joblib ë³‘ë ¬ ì²˜ë¦¬ ì¤€ë¹„ ì™„ë£Œ")

    def setup_cache_directory(self):
        """ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •"""
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
            print(f"ğŸ“ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {self.cache_dir}")

    def setup_resource_limits(self):
        """ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì •"""
        # CPU ì œí•œ (ë¬¼ë¦¬ ì½”ì–´ ê¸°ì¤€)
        total_cpus = psutil.cpu_count(logical=False)
        self.max_workers = max(1, total_cpus)

        # ë©”ëª¨ë¦¬ ì œí•œ (70%)
        total_memory = psutil.virtual_memory().total
        self.max_memory = int(total_memory * 0.7)
        self.max_memory_gb = self.max_memory / (1024**3)

        # ë°°ì¹˜ í¬ê¸° ê³„ì‚° (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        estimated_memory_per_batch = 100 * 1024 * 1024  # 100MB per batch
        self.max_batch_size = max(1024, self.max_memory // estimated_memory_per_batch)

    def load_data(self, file_path: str, symbol: str = "ETHUSDT", timeframe: str = "15m") -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° Parquet ìºì‹œ"""
        cache_key = f"{symbol}_{timeframe}"
        parquet_path = os.path.join(self.cache_dir, f"{cache_key}.parquet")

        # ìºì‹œëœ Parquet íŒŒì¼ í™•ì¸
        if os.path.exists(parquet_path):
            print(f"ğŸ“¦ ìºì‹œëœ ë°ì´í„° ë¡œë“œ: {parquet_path}")
            df = pd.read_parquet(parquet_path)
        else:
            print(f"ğŸ“Š ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° ìºì‹œ ìƒì„±: {file_path}")
            df = pd.read_csv(file_path)

            # ë°ì´í„° íƒ€ì… ìµœì í™”
            df = self._optimize_dtypes(df)

            # Parquetìœ¼ë¡œ ì €ì¥
            df.to_parquet(parquet_path, compression="snappy", index=False)
            print(f"ğŸ’¾ Parquet ìºì‹œ ì €ì¥: {parquet_path}")

        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        if "time" in df.columns:
            df["time"] = pd.to_datetime(df["time"])

        # ì •ë ¬ ë° ì¸ë±ìŠ¤ ë¦¬ì…‹
        df = df.sort_values("time").reset_index(drop=True)

        # ë©”ëª¨ë¦¬ ìºì‹œ
        self.cached_data[cache_key] = df

        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ í–‰")
        return df

    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° íƒ€ì… ìµœì í™” - float32 ë‹¤ìš´ìºìŠ¤íŒ…"""
        print("ğŸ”§ ë°ì´í„° íƒ€ì… ìµœì í™” ì¤‘...")

        # OHLCV ì»¬ëŸ¼ì„ float32ë¡œ ë³€í™˜
        float_columns = ["open", "high", "low", "close", "volume"]
        for col in float_columns:
            if col in df.columns:
                df[col] = df[col].astype(np.float32)

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f}MB")

        return df

    def cache_indicators(self, df: pd.DataFrame, params: Dict) -> Dict[str, np.ndarray]:
        """ì§€í‘œ ì‚¬ì „ê³„ì‚° ë° ndarray ìºì‹œ"""
        print("âš¡ ì§€í‘œ ì‚¬ì „ê³„ì‚° ì¤‘...")

        # ê¸°ë³¸ ë°°ì—´ ì¶”ì¶œ
        high = df["high"].values.astype(np.float32)
        low = df["low"].values.astype(np.float32)
        close = df["close"].values.astype(np.float32)
        open_price = df["open"].values.astype(np.float32)
        volume = df["volume"].values.astype(np.float32)

        # ì‹œê°„ ì •ë³´
        time_values = df["time"].values
        hours = pd.to_datetime(time_values).hour.values.astype(np.int8)
        minutes = pd.to_datetime(time_values).minute.values.astype(np.int8)
        weekdays = pd.to_datetime(time_values).weekday.values.astype(np.int8)

        # Numba JIT ì»´íŒŒì¼ëœ ì§€í‘œ ê³„ì‚°
        indicators = {}

        # ATR ê³„ì‚°
        atr_len = params.get("atr_len", 41)
        indicators["atr"] = self._calculate_atr_numba(high, low, close, atr_len)

        # True Range
        indicators["tr"] = self._calculate_tr_numba(high, low, close)

        # ìŠ¤ìœ™ í¬ì¸íŠ¸
        swing_len = params.get("swing_len", 3)
        swing_highs, swing_lows = self._find_swing_points_numba(high, low, swing_len)
        indicators["swing_high"] = swing_highs
        indicators["swing_low"] = swing_lows

        # ì„¸ì…˜ êµ¬ë¶„
        indicators["session"] = self._identify_sessions_numba(hours)

        # ë°”ë”” í¬ê¸°
        indicators["body"] = np.abs(close - open_price)
        indicators["body_pct"] = indicators["body"] / (high - low + 1e-8)

        # ë””ìŠ¤í”Œë ˆì´ìŠ¤ë¨¼íŠ¸
        disp_mult = params.get("disp_mult", 1.31)
        indicators["displacement"] = self._calculate_displacement_numba(open_price, close, high, low, disp_mult)

        # ì¼ì¤‘ ë³€ë™ì„± (ê°„ì†Œí™”ëœ ë²„ì „)
        indicators["rr_percentile"] = self._calculate_rr_percentile_numba(indicators["tr"], params.get("rr_percentile", 0.13))

        # ì‹œê°„ ì •ë³´ ì €ì¥
        indicators["hour"] = hours
        indicators["minute"] = minutes
        indicators["weekday"] = weekdays

        # ê¸°ë³¸ ê°€ê²© ë°ì´í„°
        indicators["open"] = open_price
        indicators["high"] = high
        indicators["low"] = low
        indicators["close"] = close
        indicators["volume"] = volume

        self.cached_indicators = indicators

        print(f"âœ… {len(indicators)}ê°œ ì§€í‘œ ìºì‹œ ì™„ë£Œ")
        return indicators

    @staticmethod
    @njit
    def _calculate_atr_numba(high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int) -> np.ndarray:
        """Numba JIT ATR ê³„ì‚°"""
        n = len(high)
        tr = np.zeros(n, dtype=np.float32)
        atr = np.zeros(n, dtype=np.float32)

        # True Range ê³„ì‚°
        for i in range(1, n):
            tr1 = high[i] - low[i]
            tr2 = abs(high[i] - close[i - 1])
            tr3 = abs(low[i] - close[i - 1])
            tr[i] = max(tr1, max(tr2, tr3))

        # ATR ê³„ì‚° (Simple Moving Average)
        for i in range(period, n):
            atr[i] = np.mean(tr[i - period + 1 : i + 1])

        return atr

    @staticmethod
    @njit
    def _calculate_tr_numba(high: np.ndarray, low: np.ndarray, close: np.ndarray) -> np.ndarray:
        """Numba JIT True Range ê³„ì‚°"""
        n = len(high)
        tr = np.zeros(n, dtype=np.float32)

        for i in range(1, n):
            tr1 = high[i] - low[i]
            tr2 = abs(high[i] - close[i - 1])
            tr3 = abs(low[i] - close[i - 1])
            tr[i] = max(tr1, max(tr2, tr3))

        return tr

    @staticmethod
    @njit
    def _find_swing_points_numba(high: np.ndarray, low: np.ndarray, swing_len: int) -> Tuple[np.ndarray, np.ndarray]:
        """Numba JIT ìŠ¤ìœ™ í¬ì¸íŠ¸ ì°¾ê¸°"""
        n = len(high)
        swing_highs = np.zeros(n, dtype=np.bool_)
        swing_lows = np.zeros(n, dtype=np.bool_)

        for i in range(swing_len, n - swing_len):
            # ìŠ¤ìœ™ í•˜ì´ í™•ì¸
            is_high = True
            for k in range(1, swing_len + 1):
                if high[i] <= high[i - k] or high[i] <= high[i + k]:
                    is_high = False
                    break
            swing_highs[i] = is_high

            # ìŠ¤ìœ™ ë¡œìš° í™•ì¸
            is_low = True
            for k in range(1, swing_len + 1):
                if low[i] >= low[i - k] or low[i] >= low[i + k]:
                    is_low = False
                    break
            swing_lows[i] = is_low

        return swing_highs, swing_lows

    @staticmethod
    @njit
    def _identify_sessions_numba(hours: np.ndarray) -> np.ndarray:
        """Numba JIT ì„¸ì…˜ êµ¬ë¶„ (ê°„ì†Œí™”)"""
        n = len(hours)
        sessions = np.zeros(n, dtype=np.int8)

        # ì„¸ì…˜ ì½”ë“œ: 0=other, 1=asia, 2=london, 3=ny, 4=london_ny
        for i in range(n):
            hour = hours[i]
            if 0 <= hour < 8:  # Asia
                sessions[i] = 1
            elif 8 <= hour < 13:  # London
                sessions[i] = 2
            elif 13 <= hour < 16:  # London-NY overlap
                sessions[i] = 4
            elif 16 <= hour < 21:  # NY
                sessions[i] = 3
            else:  # Other
                sessions[i] = 0

        return sessions

    @staticmethod
    @njit
    def _calculate_displacement_numba(
        open_price: np.ndarray, close: np.ndarray, high: np.ndarray, low: np.ndarray, disp_mult: float
    ) -> np.ndarray:
        """Numba JIT ë””ìŠ¤í”Œë ˆì´ìŠ¤ë¨¼íŠ¸ ê³„ì‚°"""
        n = len(open_price)
        displacement = np.zeros(n, dtype=np.bool_)

        # ë°”ë””ì™€ ë ˆì¸ì§€ í¬ê¸°
        body = np.abs(close - open_price)
        range_size = high - low

        # 10ë°” ì´ë™í‰ê·  ê³„ì‚°
        window = 10
        for i in range(window, n):
            avg_body = np.mean(body[i - window : i])
            avg_range = np.mean(range_size[i - window : i])

            body_disp = body[i] >= (disp_mult * avg_body)
            range_disp = range_size[i] >= (disp_mult * avg_range)

            displacement[i] = body_disp or range_disp

        return displacement

    @staticmethod
    @njit
    def _calculate_rr_percentile_numba(tr: np.ndarray, threshold: float) -> np.ndarray:
        """Numba JIT RR Percentile ê³„ì‚° (ê°„ì†Œí™”)"""
        n = len(tr)
        rr_percentile = np.zeros(n, dtype=np.float32)

        # 20ì¼ ë¡¤ë§ ìœˆë„ìš°
        window = 20 * 96  # 20ì¼ * 96ê°œ 15ë¶„ë´‰

        for i in range(window, n):
            # í˜„ì¬ TRê³¼ ê³¼ê±° TR ë¹„êµ
            current_tr = tr[i]
            past_trs = tr[i - window : i]

            # í¼ì„¼íƒ€ì¼ ê³„ì‚°
            count_below = 0
            for j in range(len(past_trs)):
                if past_trs[j] < current_tr:
                    count_below += 1

            rr_percentile[i] = count_below / len(past_trs)

        return rr_percentile

    def get_cached_slice(self, start: int, end: int, indicators: List[str] = None) -> Dict[str, np.ndarray]:
        """ìºì‹œëœ ë°ì´í„° ìŠ¬ë¼ì´ìŠ¤ ë°˜í™˜"""
        if not self.cached_indicators:
            raise ValueError("ì§€í‘œê°€ ìºì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. cache_indicators()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

        if indicators is None:
            indicators = list(self.cached_indicators.keys())

        sliced_data = {}
        for indicator in indicators:
            if indicator in self.cached_indicators:
                sliced_data[indicator] = self.cached_indicators[indicator][start:end]

        return sliced_data

    def update_incremental(self, new_bar: Dict) -> None:
        """ì¦ë¶„ ì—…ë°ì´íŠ¸ (ì‹¤ì‹œê°„ ë°ì´í„°ìš©)"""
        # ìƒˆë¡œìš´ ë°” ë°ì´í„°ë¥¼ ìºì‹œì— ì¶”ê°€
        for key, value in new_bar.items():
            if key in self.cached_indicators:
                # numpy arrayì— ìƒˆ ê°’ ì¶”ê°€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ì§€ ì•ŠìŒ, ì‹¤ì œë¡œëŠ” circular buffer ì‚¬ìš©)
                self.cached_indicators[key] = np.append(self.cached_indicators[key], value)

    def parallel_backtest(self, param_sets: List[Dict], strategy_func, n_jobs: int = None) -> List[Dict]:
        """ë³‘ë ¬ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if n_jobs is None:
            n_jobs = self.max_workers

        print(f"ğŸ”„ ë³‘ë ¬ ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(param_sets)}ê°œ íŒŒë¼ë¯¸í„° ì„¸íŠ¸, {n_jobs}ê°œ í”„ë¡œì„¸ìŠ¤")

        if RAY_AVAILABLE:
            return self._parallel_backtest_ray(param_sets, strategy_func)
        elif JOBLIB_AVAILABLE:
            return self._parallel_backtest_joblib(param_sets, strategy_func, n_jobs)
        else:
            # ë‹¨ì¼ ìŠ¤ë ˆë“œ í´ë°±
            return [strategy_func(params) for params in param_sets]

    def _parallel_backtest_ray(self, param_sets: List[Dict], strategy_func) -> List[Dict]:
        """Rayë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ë°±í…ŒìŠ¤íŠ¸"""
        try:
            import ray

            @ray.remote
            def remote_backtest(params):
                return strategy_func(params)

            # Ray ì‘ì—… ì œì¶œ
            futures = [remote_backtest.remote(params) for params in param_sets]

            # ê²°ê³¼ ìˆ˜ì§‘
            results = ray.get(futures)
            return results
        except ImportError:
            # Rayê°€ ì—†ìœ¼ë©´ ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œ í´ë°±
            return [strategy_func(params) for params in param_sets]

    def _parallel_backtest_joblib(self, param_sets: List[Dict], strategy_func, n_jobs: int) -> List[Dict]:
        """Joblibì„ ì‚¬ìš©í•œ ë³‘ë ¬ ë°±í…ŒìŠ¤íŠ¸"""
        results = Parallel(n_jobs=n_jobs, backend="multiprocessing")(delayed(strategy_func)(params) for params in param_sets)
        return results

    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        memory_info = {}

        # ìºì‹œëœ ë°ì´í„° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        total_cache_memory = 0
        for key, data in self.cached_indicators.items():
            if isinstance(data, np.ndarray):
                memory_mb = data.nbytes / 1024 / 1024
                memory_info[f"cache_{key}"] = memory_mb
                total_cache_memory += memory_mb

        memory_info["total_cache_mb"] = total_cache_memory
        memory_info["system_available_mb"] = psutil.virtual_memory().available / 1024 / 1024
        memory_info["system_used_pct"] = psutil.virtual_memory().percent

        return memory_info

    def cleanup_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        self.cached_data.clear()
        self.cached_indicators.clear()

        # Ray ì •ë¦¬ ë¶ˆí•„ìš” (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)

        print("ğŸ§¹ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    engine = FastDataEngine()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_file = "data/ETHUSDT_15m_206319points_20251015_202539.csv"
    if os.path.exists(test_file):
        df = engine.load_data(test_file)

        # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
        test_params = {"atr_len": 41, "swing_len": 3, "disp_mult": 1.31, "rr_percentile": 0.13}

        # ì§€í‘œ ìºì‹œ
        indicators = engine.cache_indicators(df, test_params)

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_usage = engine.get_memory_usage()
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        for key, value in memory_usage.items():
            print(f"   {key}: {value:.1f}MB")

        # ìŠ¬ë¼ì´ìŠ¤ í…ŒìŠ¤íŠ¸
        slice_data = engine.get_cached_slice(1000, 2000, ["atr", "close", "displacement"])
        print(f"\nğŸ” ìŠ¬ë¼ì´ìŠ¤ í…ŒìŠ¤íŠ¸: {len(slice_data)}ê°œ ì§€í‘œ, {len(slice_data['atr'])}ê°œ ë°ì´í„° í¬ì¸íŠ¸")

        engine.cleanup_cache()
    else:
        print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_file}")


if __name__ == "__main__":
    main()
