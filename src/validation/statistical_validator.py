#!/usr/bin/env python3
"""
í†µê³„ì  ê²€ì¦ ì‹œìŠ¤í…œ êµ¬í˜„
- Deflated Sortino (Bailey) ë‹¤ì¤‘ê°€ì„¤ ë³´ì •
- White's Reality Check / SPA ìš°ì—°ì„± ê²€ì •
- 0.6Â·(MC p5) + 0.4Â·(WFO-OOS median) ê°€ì¤‘í•© ì„ íƒ
- Top-1~2 ì‹œìŠ¤í…œ ìµœì¢… ì„ íƒ
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

from ..core.performance_evaluator import PerformanceEvaluator, PerformanceMetrics
from montecarlo_simulator import MonteCarloResult
from walkforward_analyzer import WalkForwardResult

@dataclass
class StatisticalTestResult:
    """í†µê³„ì  ê²€ì • ê²°ê³¼"""
    test_name: str
    statistic: float
    p_value: float
    critical_value: float
    passed: bool
    confidence_level: float

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    candidate_id: int
    params: Dict
    wfo_score: float
    mc_score: float
    combined_score: float
    statistical_tests: List[StatisticalTestResult]
    deflated_sortino: float
    reality_check_passed: bool
    spa_test_passed: bool
    final_ranking: int
    recommended: bool

class StatisticalValidator:
    def __init__(self, performance_evaluator: PerformanceEvaluator):
        """í†µê³„ì  ê²€ì¦ì ì´ˆê¸°í™”"""
        self.performance_evaluator = performance_evaluator
        
        # ê²€ì • ì„¤ì •
        self.test_config = {
            'confidence_level': 0.05,      # 5% ìœ ì˜ìˆ˜ì¤€
            'deflated_threshold': 0.05,    # Deflated Sortino ì„ê³„ê°’
            'reality_check_alpha': 0.05,   # Reality Check ìœ ì˜ìˆ˜ì¤€
            'spa_alpha': 0.05,             # SPA í…ŒìŠ¤íŠ¸ ìœ ì˜ìˆ˜ì¤€
            'bootstrap_samples': 1000      # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ ìˆ˜
        }
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        self.weight_config = {
            'mc_weight': 0.6,              # ëª¬í…Œì¹´ë¥¼ë¡œ ê°€ì¤‘ì¹˜
            'wfo_weight': 0.4              # ì›Œí¬í¬ì›Œë“œ ê°€ì¤‘ì¹˜
        }
        
        print("ğŸ“Š í†µê³„ì  ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"   ìœ ì˜ìˆ˜ì¤€: {self.test_config['confidence_level']*100}%")
        print(f"   ê°€ì¤‘ì¹˜: MC({self.weight_config['mc_weight']}) + WFO({self.weight_config['wfo_weight']})")
    
    def calculate_deflated_sortino(self, sortino_ratio: float, 
                                 n_tests: int, n_observations: int) -> Tuple[float, bool]:
        """Deflated Sortino Ratio ê³„ì‚° (Bailey et al.)"""
        if n_tests <= 1 or n_observations <= 1:
            return sortino_ratio, True
        
        # ë‹¤ì¤‘ í…ŒìŠ¤íŠ¸ ë³´ì •
        # Baileyì˜ ê³µì‹: DSR = SR * sqrt((1 - Î³) / (1 - Î³ * Ï))
        # ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”ëœ ë²„ì „ ì‚¬ìš©
        
        # ë…ë¦½ì„± ê°€ì •í•˜ì— ë³´ì • ê³„ìˆ˜ ê³„ì‚°
        correction_factor = np.sqrt(np.log(n_tests))
        
        # Deflated Sortino Ratio
        deflated_sortino = sortino_ratio / correction_factor
        
        # ì„ê³„ê°’ê³¼ ë¹„êµ
        threshold = stats.norm.ppf(1 - self.test_config['deflated_threshold'])
        passed = deflated_sortino >= threshold
        
        print(f"ğŸ“‰ Deflated Sortino: {deflated_sortino:.4f} (ì›ë³¸: {sortino_ratio:.4f})")
        print(f"   í…ŒìŠ¤íŠ¸ ìˆ˜: {n_tests}, ê´€ì¸¡ì¹˜: {n_observations}")
        print(f"   ì„ê³„ê°’: {threshold:.4f}, í†µê³¼: {'âœ…' if passed else 'âŒ'}")
        
        return deflated_sortino, passed
    
    def whites_reality_check(self, benchmark_returns: np.ndarray, 
                           strategy_returns: List[np.ndarray]) -> Tuple[float, bool]:
        """White's Reality Check ê²€ì •"""
        if len(strategy_returns) == 0:
            return 0.0, False
        
        n_strategies = len(strategy_returns)
        n_obs = len(benchmark_returns)
        
        # ê° ì „ëµì˜ ì´ˆê³¼ ìˆ˜ìµë¥  ê³„ì‚°
        excess_returns = []
        for strategy_ret in strategy_returns:
            if len(strategy_ret) == len(benchmark_returns):
                excess = strategy_ret - benchmark_returns
                excess_returns.append(excess)
        
        if len(excess_returns) == 0:
            return 0.0, False
        
        # ìµœëŒ€ ì´ˆê³¼ ìˆ˜ìµë¥ 
        max_excess = np.max([np.mean(excess) for excess in excess_returns])
        
        # ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¶„í¬ ìƒì„±
        bootstrap_max = []
        for _ in range(self.test_config['bootstrap_samples']):
            # ì‹œê°„ ìˆœì„œë¥¼ ìœ ì§€í•œ ë¸”ë¡ ë¶€íŠ¸ìŠ¤íŠ¸ë©
            block_size = max(1, int(np.sqrt(n_obs)))
            n_blocks = (n_obs + block_size - 1) // block_size
            
            bootstrap_excess = []
            for excess in excess_returns:
                boot_series = []
                for _ in range(n_blocks):
                    start_idx = np.random.randint(0, max(1, len(excess) - block_size + 1))
                    block = excess[start_idx:start_idx + block_size]
                    boot_series.extend(block)
                
                boot_series = np.array(boot_series[:n_obs])
                # í‰ê· ì„ 0ìœ¼ë¡œ ì¡°ì • (ê·€ë¬´ê°€ì„¤ í•˜ì—ì„œ)
                boot_series = boot_series - np.mean(boot_series)
                bootstrap_excess.append(np.mean(boot_series))
            
            bootstrap_max.append(np.max(bootstrap_excess))
        
        # p-value ê³„ì‚°
        p_value = np.mean(np.array(bootstrap_max) >= max_excess)
        passed = p_value <= self.test_config['reality_check_alpha']
        
        print(f"ğŸ¯ White's Reality Check:")
        print(f"   ìµœëŒ€ ì´ˆê³¼ìˆ˜ìµë¥ : {max_excess:.6f}")
        print(f"   p-value: {p_value:.4f}")
        print(f"   í†µê³¼: {'âœ…' if passed else 'âŒ'}")
        
        return p_value, passed
    
    def spa_test(self, benchmark_returns: np.ndarray, 
                strategy_returns: List[np.ndarray]) -> Tuple[float, bool]:
        """Superior Predictive Ability (SPA) í…ŒìŠ¤íŠ¸"""
        if len(strategy_returns) == 0:
            return 0.0, False
        
        n_strategies = len(strategy_returns)
        n_obs = len(benchmark_returns)
        
        # ê° ì „ëµì˜ ì´ˆê³¼ ìˆ˜ìµë¥  ê³„ì‚°
        excess_returns = []
        for strategy_ret in strategy_returns:
            if len(strategy_ret) == len(benchmark_returns):
                excess = strategy_ret - benchmark_returns
                excess_returns.append(excess)
        
        if len(excess_returns) == 0:
            return 0.0, False
        
        # í‰ê·  ì´ˆê³¼ ìˆ˜ìµë¥ ê³¼ í‘œì¤€ì˜¤ì°¨
        mean_excess = np.array([np.mean(excess) for excess in excess_returns])
        std_excess = np.array([np.std(excess) / np.sqrt(len(excess)) for excess in excess_returns])
        
        # t-í†µê³„ëŸ‰ ê³„ì‚°
        t_stats = mean_excess / (std_excess + 1e-8)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        max_t_stat = np.max(t_stats)
        
        # ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¶„í¬ (SPA ë°©ë²•)
        bootstrap_max_t = []
        for _ in range(self.test_config['bootstrap_samples']):
            boot_t_stats = []
            
            for i, excess in enumerate(excess_returns):
                # í‰ê·  ì¤‘ì‹¬í™”ëœ ë¶€íŠ¸ìŠ¤íŠ¸ë©
                centered_excess = excess - np.mean(excess)
                boot_sample = np.random.choice(centered_excess, size=len(excess), replace=True)
                
                boot_mean = np.mean(boot_sample)
                boot_std = np.std(boot_sample) / np.sqrt(len(boot_sample))
                
                # ì›ë˜ í‘œì¤€ì˜¤ì°¨ë¡œ ì •ê·œí™” (SPAì˜ í•µì‹¬)
                boot_t = boot_mean / (std_excess[i] + 1e-8)
                boot_t_stats.append(boot_t)
            
            bootstrap_max_t.append(np.max(boot_t_stats))
        
        # p-value ê³„ì‚°
        p_value = np.mean(np.array(bootstrap_max_t) >= max_t_stat)
        passed = p_value <= self.test_config['spa_alpha']
        
        print(f"ğŸ”¬ SPA Test:")
        print(f"   ìµœëŒ€ t-í†µê³„ëŸ‰: {max_t_stat:.4f}")
        print(f"   p-value: {p_value:.4f}")
        print(f"   í†µê³¼: {'âœ…' if passed else 'âŒ'}")
        
        return p_value, passed
    
    def calculate_combined_score(self, wfo_result: WalkForwardResult, 
                               mc_result: MonteCarloResult) -> float:
        """ê°€ì¤‘ ê²°í•© ì ìˆ˜ ê³„ì‚°"""
        # WFO OOS ë©”ë””ì•ˆ ì ìˆ˜
        wfo_score = wfo_result.median_score
        
        # MC 5ë¶„ìœ„ ì ìˆ˜ (ê²¬ê³ ì„± ê¸°ì¤€)
        mc_score = mc_result.robustness_score
        
        # ê°€ì¤‘ ê²°í•©
        combined_score = (self.weight_config['mc_weight'] * mc_score + 
                         self.weight_config['wfo_weight'] * wfo_score)
        
        return combined_score
    
    def validate_candidates(self, candidates: List[Tuple[Dict, WalkForwardResult, MonteCarloResult]]) -> List[ValidationResult]:
        """í›„ë³´ë“¤ì— ëŒ€í•œ í†µê³„ì  ê²€ì¦ ì‹¤í–‰"""
        print(f"\nğŸ“Š í†µê³„ì  ê²€ì¦ ì‹œì‘ ({len(candidates)}ê°œ í›„ë³´)")
        
        validation_results = []
        
        # ë²¤ì¹˜ë§ˆí¬ ìˆ˜ìµë¥  (ì²« ë²ˆì§¸ í›„ë³´ë¥¼ ë²¤ì¹˜ë§ˆí¬ë¡œ ì‚¬ìš©)
        if candidates:
            benchmark_returns = self._extract_returns(candidates[0][1])
        else:
            benchmark_returns = np.array([])
        
        # ëª¨ë“  ì „ëµì˜ ìˆ˜ìµë¥  ìˆ˜ì§‘
        all_strategy_returns = []
        for params, wfo_result, mc_result in candidates:
            strategy_returns = self._extract_returns(wfo_result)
            all_strategy_returns.append(strategy_returns)
        
        # ê° í›„ë³´ ê²€ì¦
        for i, (params, wfo_result, mc_result) in enumerate(candidates):
            print(f"\nğŸ” í›„ë³´ {i+1} ê²€ì¦ ì¤‘...")
            
            # ê²°í•© ì ìˆ˜ ê³„ì‚°
            combined_score = self.calculate_combined_score(wfo_result, mc_result)
            
            # í†µê³„ì  ê²€ì •ë“¤
            statistical_tests = []
            
            # 1. Deflated Sortino
            original_sortino = wfo_result.aggregated_metrics.sortino_ratio
            deflated_sortino, deflated_passed = self.calculate_deflated_sortino(
                original_sortino, len(candidates), len(wfo_result.slices)
            )
            
            statistical_tests.append(StatisticalTestResult(
                test_name="Deflated Sortino",
                statistic=deflated_sortino,
                p_value=0.0,  # N/A for this test
                critical_value=stats.norm.ppf(1 - self.test_config['deflated_threshold']),
                passed=deflated_passed,
                confidence_level=self.test_config['confidence_level']
            ))
            
            # 2. White's Reality Check
            wrc_p_value, wrc_passed = self.whites_reality_check(
                benchmark_returns, all_strategy_returns
            )
            
            statistical_tests.append(StatisticalTestResult(
                test_name="White's Reality Check",
                statistic=0.0,  # N/A
                p_value=wrc_p_value,
                critical_value=self.test_config['reality_check_alpha'],
                passed=wrc_passed,
                confidence_level=self.test_config['confidence_level']
            ))
            
            # 3. SPA Test
            spa_p_value, spa_passed = self.spa_test(
                benchmark_returns, all_strategy_returns
            )
            
            statistical_tests.append(StatisticalTestResult(
                test_name="SPA Test",
                statistic=0.0,  # N/A
                p_value=spa_p_value,
                critical_value=self.test_config['spa_alpha'],
                passed=spa_passed,
                confidence_level=self.test_config['confidence_level']
            ))
            
            # ê²€ì¦ ê²°ê³¼ ìƒì„±
            result = ValidationResult(
                candidate_id=i,
                params=params,
                wfo_score=wfo_result.median_score,
                mc_score=mc_result.robustness_score,
                combined_score=combined_score,
                statistical_tests=statistical_tests,
                deflated_sortino=deflated_sortino,
                reality_check_passed=wrc_passed,
                spa_test_passed=spa_passed,
                final_ranking=0,  # ë‚˜ì¤‘ì— ì„¤ì •
                recommended=False  # ë‚˜ì¤‘ì— ì„¤ì •
            )
            
            validation_results.append(result)
        
        # ìµœì¢… ë­í‚¹ ë° ì¶”ì²œ ì„¤ì •
        validation_results = self._rank_and_recommend(validation_results)
        
        print(f"\nâœ… í†µê³„ì  ê²€ì¦ ì™„ë£Œ")
        return validation_results
    
    def _extract_returns(self, wfo_result: WalkForwardResult) -> np.ndarray:
        """ì›Œí¬í¬ì›Œë“œ ê²°ê³¼ì—ì„œ ìˆ˜ìµë¥  ì¶”ì¶œ"""
        returns = []
        for slice_obj in wfo_result.slices:
            if slice_obj.oos_metrics and slice_obj.oos_metrics.total_return:
                returns.append(slice_obj.oos_metrics.total_return)
        
        return np.array(returns) if returns else np.array([0])
    
    def _rank_and_recommend(self, results: List[ValidationResult]) -> List[ValidationResult]:
        """ê²°ê³¼ ë­í‚¹ ë° ì¶”ì²œ ì„¤ì •"""
        # í†µê³„ì  ê²€ì •ì„ ëª¨ë‘ í†µê³¼í•œ í›„ë³´ë“¤ë§Œ í•„í„°ë§
        valid_results = []
        for result in results:
            all_tests_passed = all(test.passed for test in result.statistical_tests)
            if all_tests_passed:
                valid_results.append(result)
        
        if not valid_results:
            print("âš ï¸ ëª¨ë“  í†µê³„ì  ê²€ì •ì„ í†µê³¼í•œ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
            # ê·¸ë˜ë„ ìµœê³  ì ìˆ˜ í›„ë³´ëŠ” í¬í•¨
            if results:
                best_result = max(results, key=lambda x: x.combined_score)
                valid_results = [best_result]
        
        # ê²°í•© ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        valid_results.sort(key=lambda x: x.combined_score, reverse=True)
        
        # ë­í‚¹ ì„¤ì •
        for i, result in enumerate(valid_results):
            result.final_ranking = i + 1
        
        # Top-1~2ë§Œ ì¶”ì²œ
        for i, result in enumerate(valid_results):
            result.recommended = i < 2  # Top-2
        
        # ì›ë˜ ë¦¬ìŠ¤íŠ¸ì— ì—…ë°ì´íŠ¸
        for original_result in results:
            for valid_result in valid_results:
                if original_result.candidate_id == valid_result.candidate_id:
                    original_result.final_ranking = valid_result.final_ranking
                    original_result.recommended = valid_result.recommended
                    break
        
        return results
    
    def print_validation_results(self, results: List[ValidationResult], 
                               title: str = "í†µê³„ì  ê²€ì¦ ê²°ê³¼"):
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ“Š {title}")
        print("=" * 80)
        
        # ì¶”ì²œ í›„ë³´ë“¤ë§Œ ë¨¼ì € ì¶œë ¥
        recommended = [r for r in results if r.recommended]
        if recommended:
            print(f"ğŸ† ì¶”ì²œ í›„ë³´ ({len(recommended)}ê°œ):")
            for result in recommended:
                self._print_single_result(result)
        
        # ë‚˜ë¨¸ì§€ í›„ë³´ë“¤
        others = [r for r in results if not r.recommended]
        if others:
            print(f"\nğŸ“‹ ê¸°íƒ€ í›„ë³´ ({len(others)}ê°œ):")
            for result in others[:3]:  # ìƒìœ„ 3ê°œë§Œ
                self._print_single_result(result, brief=True)
    
    def _print_single_result(self, result: ValidationResult, brief: bool = False):
        """ë‹¨ì¼ ê²°ê³¼ ì¶œë ¥"""
        status = "ğŸŒŸ ì¶”ì²œ" if result.recommended else f"#{result.final_ranking}"
        
        print(f"\n{status} í›„ë³´ {result.candidate_id + 1}:")
        print(f"   ê²°í•© ì ìˆ˜: {result.combined_score:.4f}")
        print(f"   WFO ì ìˆ˜: {result.wfo_score:.4f}")
        print(f"   MC ì ìˆ˜: {result.mc_score:.4f}")
        
        if not brief:
            # í†µê³„ì  ê²€ì • ê²°ê³¼
            print(f"   í†µê³„ì  ê²€ì •:")
            for test in result.statistical_tests:
                status_icon = "âœ…" if test.passed else "âŒ"
                if test.test_name == "Deflated Sortino":
                    print(f"     {test.test_name}: {test.statistic:.4f} {status_icon}")
                else:
                    print(f"     {test.test_name}: p={test.p_value:.4f} {status_icon}")
            
            # ì£¼ìš” íŒŒë¼ë¯¸í„°
            key_params = ['target_r', 'stop_atr_mult', 'swing_len', 'rr_percentile']
            param_str = ", ".join([f"{k}: {result.params.get(k, 'N/A'):.3f}" if isinstance(result.params.get(k), float) 
                                 else f"{k}: {result.params.get(k, 'N/A')}" for k in key_params])
            print(f"   íŒŒë¼ë¯¸í„°: {param_str}")
    
    def get_final_recommendations(self, results: List[ValidationResult]) -> List[ValidationResult]:
        """ìµœì¢… ì¶”ì²œ í›„ë³´ ë°˜í™˜"""
        return [r for r in results if r.recommended]

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    # ì„±ê³¼ í‰ê°€ì ì´ˆê¸°í™”
    performance_evaluator = PerformanceEvaluator()
    
    # í†µê³„ì  ê²€ì¦ì ì´ˆê¸°í™”
    validator = StatisticalValidator(performance_evaluator)
    
    # ê°€ìƒì˜ í›„ë³´ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ë°›ì•„ì˜´)
    from walkforward_analyzer import WalkForwardResult, WalkForwardSlice
    from montecarlo_simulator import MonteCarloResult
    
    # í…ŒìŠ¤íŠ¸ìš© ê°€ìƒ ë°ì´í„°
    test_candidates = []
    
    for i in range(3):
        # ê°€ìƒ íŒŒë¼ë¯¸í„°
        params = {
            'target_r': 2.5 + i * 0.2,
            'stop_atr_mult': 0.1 + i * 0.01,
            'swing_len': 5 + i,
            'rr_percentile': 0.2 + i * 0.05
        }
        
        # ê°€ìƒ WFO ê²°ê³¼
        slices = []
        for j in range(8):
            slice_obj = WalkForwardSlice(
                slice_id=j+1, train_start=0, train_end=1000, test_start=1000, test_end=1200,
                train_period="", test_period="", regime="normal"
            )
            slice_obj.oos_metrics = performance_evaluator._empty_metrics()
            slice_obj.oos_metrics.total_return = np.random.normal(0.1, 0.05)  # 10% Â± 5%
            slice_obj.oos_score = np.random.normal(0.5, 0.1)
            slices.append(slice_obj)
        
        wfo_result = WalkForwardResult(
            slices=slices,
            aggregated_metrics=performance_evaluator._empty_metrics(),
            median_score=0.5 + i * 0.1,
            consistency_ratio=0.8,
            regime_performance={},
            passed_oos_criteria=True
        )
        wfo_result.aggregated_metrics.sortino_ratio = 2.0 + i * 0.2
        
        # ê°€ìƒ MC ê²°ê³¼
        mc_result = MonteCarloResult(
            percentiles={'profit_factor_p5': 1.5 + i * 0.1},
            stability_metrics={'profit_factor_cv': 0.1},
            robustness_score=0.7 + i * 0.1,
            passed_criteria=True,
            simulation_count=1000,
            original_metrics=performance_evaluator._empty_metrics()
        )
        
        test_candidates.append((params, wfo_result, mc_result))
    
    # í†µê³„ì  ê²€ì¦ ì‹¤í–‰
    results = validator.validate_candidates(test_candidates)
    
    # ê²°ê³¼ ì¶œë ¥
    validator.print_validation_results(results)
    
    # ìµœì¢… ì¶”ì²œ
    recommendations = validator.get_final_recommendations(results)
    print(f"\nğŸ¯ ìµœì¢… ì¶”ì²œ: {len(recommendations)}ê°œ ì‹œìŠ¤í…œ")

if __name__ == "__main__":
    main()